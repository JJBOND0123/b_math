"""B ç«™é‡‡é›†è„šæœ¬ï¼šæŒ‰é…ç½®å…³é”®è¯æŠ“å–è§†é¢‘ -> ä¼°ç®—æ ¸å¿ƒæŒ‡æ ‡ -> æ™ºèƒ½åˆ†ç±» -> å†™å…¥ MySQLã€‚"""

import os
import random
import time
from datetime import datetime

import joblib
import pymysql
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3

# é‡‡é›†æ—¶ä¼šä½¿ç”¨ verify=False è§„é¿éƒ¨åˆ†åœ°åŒºçš„è¯ä¹¦é—®é¢˜ï¼Œè¿™é‡Œæå‰å…³æ‰å‘Šè­¦ã€‚
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# === 1. åŠ è½½å¯é€‰çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼ˆæ²¡æœ‰ä¹Ÿèƒ½è¿è¡Œï¼Œé€€å›å…³é”®å­—è§„åˆ™ï¼‰ ===
MODEL_PATH = 'subject_classifier.pkl'
ML_MODEL = None
if os.path.exists(MODEL_PATH):
    try:
        ML_MODEL = joblib.load(MODEL_PATH)
        print("âœ… AI Model loaded successfully.")
    except Exception as e:
        print(f"âš ï¸ Failed to load AI model: {e}")
else:
    print("âš ï¸ Warning: AI model not found. Running in rule-based mode.")

# === 2. ç¯å¢ƒå˜é‡ / Cookie é…ç½® ===
COOKIE = """your_cookie"""
if not COOKIE:
    raise RuntimeError("Missing COOKIE.")

DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "user": os.getenv("DB_USER", "root"),
    "password": os.getenv("DB_PASSWORD", "123456"),
    "db": os.getenv("DB_NAME", "bilibili_math_db"),
    "port": int(os.getenv("DB_PORT", 3306)),
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor,
}

# === 3. å…³é”®è¯ä¸åˆ†ç±»æ˜ å°„é…ç½®ï¼ˆæ–¹ä¾¿ç­”è¾©è¯´æ˜â€œä¸ºä»€ä¹ˆä¼šæœ‰è¿™äº›åˆ†ç±»â€ï¼‰ ===
CRAWL_CONFIG = [
    # æ ¡å†…åŒæ­¥ï¼šåŸºç¡€è¯¾ä¸å…¸å‹éš¾ç‚¹
    {"q": "é«˜ç­‰æ•°å­¦ åŒæµç‰ˆ", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "å®‹æµ© é«˜æ•°", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "çº¿æ€§ä»£æ•°åŒæ­¥", "phase": "æ ¡å†…åŒæ­¥", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "å®‹æµ© çº¿æ€§ä»£æ•°", "phase": "æ ¡å†…åŒæ­¥", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ æµ™å¤§", "phase": "æ ¡å†…åŒæ­¥", "subject": "æ¦‚ç‡è®º"},
    {"q": "å®‹æµ© æ¦‚ç‡è®º", "phase": "æ ¡å†…åŒæ­¥", "subject": "æ¦‚ç‡è®º"},
    {"q": "æ³°å‹’å…¬å¼ è®²è§£", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "ä¸­å€¼å®šç†è¯æ˜", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "äºŒé‡ç§¯åˆ†", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡", "phase": "æ ¡å†…åŒæ­¥", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "æå¤§ä¼¼ç„¶ä¼°è®¡", "phase": "æ ¡å†…åŒæ­¥", "subject": "æ¦‚ç‡è®º"},
    {"q": "é«˜æ•° æœŸæœ«å¤ä¹ ", "phase": "æ ¡å†…åŒæ­¥", "subject": "æœŸæœ«çªå‡»"},
    {"q": "çº¿æ€§ä»£æ•°ä¸æŒ‚ç§‘", "phase": "æ ¡å†…åŒæ­¥", "subject": "æœŸæœ«çªå‡»"},
    {"q": "æ¦‚ç‡è®ºæœŸæœ«é€Ÿæˆ", "phase": "æ ¡å†…åŒæ­¥", "subject": "æœŸæœ«çªå‡»"},

    # å‡å­¦å¤‡è€ƒï¼šè€ƒç ”/ä¸“å‡æœ¬/åå¸ˆçŸ©é˜µ/çœŸé¢˜
    {"q": "è€ƒç ”æ•°å­¦ åŸºç¡€", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "è€ƒç ”æ•°å­¦"},
    {"q": "è€ƒç ”æ•°å­¦ å¼ºåŒ–", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "è€ƒç ”æ•°å­¦"},
    {"q": "ä¸“å‡æœ¬æ•°å­¦", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "ä¸“å‡æœ¬"},
    {"q": "å¼ å®‡ é«˜æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "å¼ å®‡"},
    {"q": "æ±¤å®¶å‡¤é«˜æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "æ±¤å®¶å‡¤"},
    {"q": "æ­¦å¿ ç¥¥é«˜æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "æ­¦å¿ ç¥¥"},
    {"q": "ææ°¸ä¹çº¿æ€§ä»£æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "ä½™ä¸™æ£®æ¦‚ç‡è®º", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "æ¦‚ç‡è®º"},
    {"q": "è€ƒç ”æ•°å­¦ çœŸé¢˜", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "çœŸé¢˜å®æˆ˜"},
    {"q": "1800é¢˜è®²è§£", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "ä¹ é¢˜ç²¾è®²"},
    {"q": "660é¢˜è®²è§£", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "ä¹ é¢˜ç²¾è®²"},

    # ç§‘æ™®ä¸ç«èµ›
    {"q": "3Blue1Brown ä¸­æ–‡", "phase": "ç›´è§‚ç§‘æ™®", "subject": "3Blue1Brown"},
    {"q": "çº¿æ€§ä»£æ•°çš„æœ¬è´¨", "phase": "ç›´è§‚ç§‘æ™®", "subject": "å¯è§†åŒ–"},
    {"q": "å¾®ç§¯åˆ†çš„æœ¬è´¨", "phase": "ç›´è§‚ç§‘æ™®", "subject": "å¯è§†åŒ–"},
    {"q": "å¤§å­¦ç”Ÿæ•°å­¦ç«èµ›", "phase": "é«˜é˜¶/ç«èµ›", "subject": "æ•°å­¦ç«èµ›"},
    {"q": "æ•°å­¦å»ºæ¨¡ å›½èµ›", "phase": "é«˜é˜¶/ç«èµ›", "subject": "æ•°å­¦å»ºæ¨¡"},
]

MAX_PAGES = 10
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Referer": "https://www.bilibili.com/",
    "Cookie": COOKIE,
}


def save_to_mysql(data_list):
    """æ‰¹é‡è½åº“ï¼šINSERT ... ON DUPLICATE KEY UPDATEï¼Œä¿è¯å­—æ®µå¯¹é½ã€‚"""
    if not data_list:
        return
    connection = pymysql.connect(**DB_CONFIG)
    try:
        with connection.cursor() as cursor:
            sql = """
            INSERT INTO videos (
                bvid, title, up_name, up_mid, up_face, pic_url, view_count, danmaku_count,
                reply_count, favorite_count, coin_count, share_count,
                duration, pubdate, tags, 
                category, phase, subject,
                dry_goods_ratio
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
                view_count = VALUES(view_count),
                favorite_count = VALUES(favorite_count),
                reply_count = VALUES(reply_count),
                dry_goods_ratio = VALUES(dry_goods_ratio),
                phase = VALUES(phase),
                subject = VALUES(subject);
            """
            values = []
            for item in data_list:
                values.append((
                    item["bvid"], item["title"], item["up_name"], item["up_mid"], item["up_face"],
                    item["pic_url"], item["view_count"], item["danmaku_count"],
                    item["reply_count"], item["favorite_count"], item["coin_count"], item["share_count"],
                    item["duration"], item["pubdate"], item["tags"],
                    item["category"], item["phase"], item["subject"],
                    item["dry_goods_ratio"],
                ))
            cursor.executemany(sql, values)
            connection.commit()
            print(f"  âœ… Saved {len(data_list)} videos -> [{data_list[0]['phase']}] - [{data_list[0]['subject']}]")
    except Exception as e:
        print(f"  âŒ DB Error: {e}")
    finally:
        connection.close()


def parse_time(timestamp):
    """B ç«™è¿”å›çš„æ˜¯æ—¶é—´æˆ³ï¼ˆç§’ï¼‰ï¼Œè½¬æˆå­—ç¬¦ä¸²æ–¹ä¾¿ MySQL DATETIMEã€‚"""
    return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")


def parse_duration(duration_str):
    """æ”¯æŒ '12:34'/'1:02:03' æˆ– int ç§’ï¼Œå¼‚å¸¸æ—¶å›è½ä¸º 0ã€‚"""
    try:
        if isinstance(duration_str, int):
            return duration_str
        if isinstance(duration_str, str) and duration_str.isdigit():
            return int(duration_str)
        parts = duration_str.split(":")
        if len(parts) == 2:
            return int(parts[0]) * 60 + int(parts[1])
        if len(parts) == 3:
            return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
        return 0
    except Exception:
        return 0


def smart_classify(title, tags, original_subject):
    """
    æ™ºèƒ½åˆ†ç±»ï¼š
    1) è‹¥å­˜åœ¨è®­ç»ƒå¥½çš„ ML æ¨¡å‹ï¼Œåˆ™ä½¿ç”¨ TF-IDF + æœ´ç´ è´å¶æ–¯åšé¢„æµ‹ï¼›
    2) é¢„æµ‹ç½®ä¿¡åº¦ä½æ—¶ï¼Œé€€å›å…³é”®è¯è§„åˆ™ï¼›ä»æœªå‘½ä¸­åˆ™è¿”å›åŸå§‹ subjectã€‚
    """
    import jieba
    if ML_MODEL:
        text = title + " " + str(tags)
        cut_text = " ".join([w for w in jieba.cut(text) if len(w) > 1])
        try:
            probs = ML_MODEL.predict_proba([cut_text])[0]
            max_prob = max(probs)
            if max_prob > 0.6:
                return ML_MODEL.predict([cut_text])[0]
        except Exception:
            pass

    combined = (title + str(tags)).lower()
    if 'çº¿ä»£' in combined or 'çº¿æ€§ä»£æ•°' in combined or 'çŸ©é˜µ' in combined:
        return 'çº¿æ€§ä»£æ•°'
    if 'é«˜æ•°' in combined or 'é«˜ç­‰æ•°å­¦' in combined or 'å¾®ç§¯åˆ†' in combined:
        return 'é«˜ç­‰æ•°å­¦'
    if 'æ¦‚ç‡' in combined or 'ç»Ÿè®¡' in combined:
        return 'æ¦‚ç‡è®º'

    return original_subject


def run_spider():
    """ä¸»æµç¨‹ï¼šéå†å…³é”®è¯ -> è°ƒç”¨æœç´¢ API -> æ¸…æ´—/è¡¥å…¨æ•°æ® -> æ‰¹é‡è½åº“ã€‚"""
    print("ğŸ•·ï¸ Spider starting...")

    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    session.mount('https://', HTTPAdapter(max_retries=retries))

    for config in CRAWL_CONFIG:
        keyword = config["q"]
        phase = config["phase"]
        subject = config["subject"]

        print(f"Fetching: {keyword} -> [{phase} - {subject}]")

        for page in range(1, MAX_PAGES + 1):
            try:
                url = "https://api.bilibili.com/x/web-interface/search/type"
                params = {"search_type": "video", "keyword": keyword, "page": page, "order": "click"}

                # éšæœºå»¶æ—¶ï¼Œé™ä½è¢«é™é¢‘æ¦‚ç‡
                time.sleep(random.uniform(2, 4))

                resp = session.get(url, headers=HEADERS, params=params, timeout=15, verify=False)
                res_json = resp.json()

                if res_json.get("code") != 0:
                    print(f"  âš ï¸ API error: {res_json.get('message')}")
                    break

                items = res_json.get("data", {}).get("result", [])
                if not items:
                    print("  No more data.")
                    break

                batch_data = []
                for item in items:
                    view = item.get("play", 0)
                    fav = item.get("favorites", 0)
                    ratio = round((fav / view * 1000), 2) if view > 0 else 0
                    mid_val = item.get("mid")
                    up_mid = int(mid_val) if mid_val else 0

                    # åˆ†ç±»ï¼šä¼˜å…ˆæœºå™¨å­¦ä¹ ï¼Œå…¶æ¬¡å…³é”®è¯è§„åˆ™
                    raw_subject = subject
                    final_subject = smart_classify(item["title"], item["tags"], raw_subject)

                    # å¡«å……ç¡¬å¸å’Œåˆ†äº«ï¼ˆB ç«™æœç´¢æ¥å£ä¸ç›´æ¥ç»™ï¼Œä¼°ç®—å€¼ç”¨äºå ä½ï¼‰
                    calc_coin = int(fav * 0.42)
                    calc_share = int(fav * 0.08)

                    video_data = {
                        "bvid": item["bvid"],
                        "title": item["title"].replace('<em class="keyword">', "").replace("</em>", ""),
                        "up_name": item["author"],
                        "up_mid": up_mid,
                        "up_face": item.get("upic") or "",
                        "pic_url": "https:" + item.get("pic", "") if item.get("pic", "").startswith("//") else item.get("pic", ""),
                        "view_count": view,
                        "danmaku_count": item.get("video_review", 0),
                        "reply_count": item.get("review", 0),
                        "favorite_count": fav,
                        "coin_count": calc_coin,
                        "share_count": calc_share,
                        "duration": parse_duration(item.get("duration", "0")),
                        "pubdate": parse_time(item.get("pubdate", time.time())),
                        "tags": keyword,
                        # åˆ†ç±»ä¿¡æ¯
                        "category": final_subject,
                        "phase": phase,
                        "subject": final_subject,
                        "dry_goods_ratio": ratio,
                    }
                    batch_data.append(video_data)

                save_to_mysql(batch_data)

            except Exception as e:
                print(f"  âŒ Exception at page {page}: {e}")
                time.sleep(5)

    print("âœ… Spider finished.")


if __name__ == "__main__":
    run_spider()
