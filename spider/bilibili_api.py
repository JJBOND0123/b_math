import os
import random
import time
from datetime import datetime
import joblib
import pymysql
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3

# ç¦ç”¨ SSL è­¦å‘Šï¼ˆå› ä¸ºæˆ‘ä»¬è¦ä½¿ç”¨ verify=False æ¥è§£å†³ç½‘ç»œé—®é¢˜ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# === 1. åŠ è½½ AI æ¨¡å‹ ===
MODEL_PATH = 'subject_classifier.pkl'
ML_MODEL = None
if os.path.exists(MODEL_PATH):
    try:
        ML_MODEL = joblib.load(MODEL_PATH)
        print("âœ… AI Model loaded successfully.")
    except Exception as e:
        print(f"âš ï¸ Failed to load AI model: {e}")
else:
    print("âš ï¸ Warning: AI model not found. Running in rule-based mode.")

# === 2. ç¯å¢ƒå˜é‡ä¸ Cookie ===
COOKIE = """your_cookie"""

if not COOKIE:
    raise RuntimeError("Missing COOKIE.")

DB_CONFIG = {
    "host": os.getenv("DB_HOST", "localhost"),
    "user": os.getenv("DB_USER", "root"),
    "password": os.getenv("DB_PASSWORD", "123456"),
    "db": os.getenv("DB_NAME", "bilibili_math_db"),
    "port": int(os.getenv("DB_PORT", 3306)),
    "charset": "utf8mb4",
    "cursorclass": pymysql.cursors.DictCursor,
}

# === 3. å‡çº§ç‰ˆå…³é”®è¯é…ç½® (90åˆ†ç‰ˆæœ¬) ===
CRAWL_CONFIG = [
    # === æ ¡å†…åŒæ­¥ ===
    {"q": "é«˜ç­‰æ•°å­¦ åŒæµç‰ˆ", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "å®‹æµ© é«˜æ•°", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "çº¿æ€§ä»£æ•° åŒæµ", "phase": "æ ¡å†…åŒæ­¥", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "å®‹æµ© çº¿æ€§ä»£æ•°", "phase": "æ ¡å†…åŒæ­¥", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ æµ™å¤§", "phase": "æ ¡å†…åŒæ­¥", "subject": "æ¦‚ç‡è®º"},
    {"q": "å®‹æµ© æ¦‚ç‡è®º", "phase": "æ ¡å†…åŒæ­¥", "subject": "æ¦‚ç‡è®º"},
    # æ ¸å¿ƒéš¾ç‚¹
    {"q": "æ³°å‹’å…¬å¼ è®²è§£", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "ä¸­å€¼å®šç† è¯æ˜", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "äºŒé‡ç§¯åˆ†", "phase": "æ ¡å†…åŒæ­¥", "subject": "é«˜ç­‰æ•°å­¦"},
    {"q": "ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡", "phase": "æ ¡å†…åŒæ­¥", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "æå¤§ä¼¼ç„¶ä¼°è®¡", "phase": "æ ¡å†…åŒæ­¥", "subject": "æ¦‚ç‡è®º"},
    # æœŸæœ«çªå‡»
    {"q": "é«˜æ•° æœŸæœ«å¤ä¹ ", "phase": "æ ¡å†…åŒæ­¥", "subject": "æœŸæœ«çªå‡»"},
    {"q": "çº¿æ€§ä»£æ•° ä¸æŒ‚ç§‘", "phase": "æ ¡å†…åŒæ­¥", "subject": "æœŸæœ«çªå‡»"},
    {"q": "æ¦‚ç‡è®º æœŸæœ«é€Ÿæˆ", "phase": "æ ¡å†…åŒæ­¥", "subject": "æœŸæœ«çªå‡»"},

    # === å‡å­¦å¤‡è€ƒ ===
    {"q": "è€ƒç ”æ•°å­¦ åŸºç¡€", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "è€ƒç ”æ•°å­¦"},
    {"q": "è€ƒç ”æ•°å­¦ å¼ºåŒ–", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "è€ƒç ”æ•°å­¦"},
    {"q": "ä¸“å‡æœ¬ æ•°å­¦", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "ä¸“å‡æœ¬"},
    # åå¸ˆçŸ©é˜µ
    {"q": "å¼ å®‡ é«˜æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "å¼ å®‡"},
    {"q": "æ±¤å®¶å‡¤ é«˜æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "æ±¤å®¶å‡¤"},
    {"q": "æ­¦å¿ ç¥¥ é«˜æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "æ­¦å¿ ç¥¥"},
    {"q": "ææ°¸ä¹ çº¿æ€§ä»£æ•°", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "çº¿æ€§ä»£æ•°"},
    {"q": "ä½™ä¸™æ£® æ¦‚ç‡è®º", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "æ¦‚ç‡è®º"},
    # ä¹ é¢˜å®æˆ˜
    {"q": "è€ƒç ”æ•°å­¦ çœŸé¢˜", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "çœŸé¢˜å®æˆ˜"},
    {"q": "1800é¢˜ è®²è§£", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "ä¹ é¢˜ç²¾è®²"},
    {"q": "660é¢˜ è®²è§£", "phase": "å‡å­¦å¤‡è€ƒ", "subject": "ä¹ é¢˜ç²¾è®²"},

    # === ç§‘æ™®ä¸ç«èµ› ===
    {"q": "3Blue1Brown ä¸­æ–‡", "phase": "ç›´è§‚ç§‘æ™®", "subject": "3Blue1Brown"},
    {"q": "çº¿æ€§ä»£æ•°çš„æœ¬è´¨", "phase": "ç›´è§‚ç§‘æ™®", "subject": "å¯è§†åŒ–"},
    {"q": "å¾®ç§¯åˆ†çš„æœ¬è´¨", "phase": "ç›´è§‚ç§‘æ™®", "subject": "å¯è§†åŒ–"},
    {"q": "å¤§å­¦ç”Ÿæ•°å­¦ç«èµ›", "phase": "é«˜é˜¶/ç«èµ›", "subject": "æ•°å­¦ç«èµ›"},
    {"q": "æ•°å­¦å»ºæ¨¡ å›½èµ›", "phase": "é«˜é˜¶/ç«èµ›", "subject": "æ•°å­¦å»ºæ¨¡"},
]

MAX_PAGES = 10
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Referer": "https://www.bilibili.com/",
    "Cookie": COOKIE,
}


def save_to_mysql(data_list):
    if not data_list:
        return
    connection = pymysql.connect(**DB_CONFIG)
    try:
        with connection.cursor() as cursor:
            # æ’å…¥è¯­å¥åŒ…å« coin_count å’Œ share_count
            sql = """
            INSERT INTO videos (
                bvid, title, up_name, up_mid, up_face, pic_url, view_count, danmaku_count,
                reply_count, favorite_count, coin_count, share_count,
                duration, pubdate, tags, 
                category, phase, subject,
                dry_goods_ratio
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
                view_count = VALUES(view_count),
                favorite_count = VALUES(favorite_count),
                reply_count = VALUES(reply_count),
                dry_goods_ratio = VALUES(dry_goods_ratio),
                phase = VALUES(phase),
                subject = VALUES(subject);
            """
            values = []
            for item in data_list:
                values.append((
                    item["bvid"], item["title"], item["up_name"], item["up_mid"], item["up_face"],
                    item["pic_url"], item["view_count"], item["danmaku_count"],
                    item["reply_count"], item["favorite_count"], item["coin_count"], item["share_count"],
                    item["duration"], item["pubdate"], item["tags"],
                    item["category"], item["phase"], item["subject"],
                    item["dry_goods_ratio"],
                ))
            cursor.executemany(sql, values)
            connection.commit()
            print(f"  âœ… Saved {len(data_list)} videos -> [{data_list[0]['phase']}] - [{data_list[0]['subject']}]")
    except Exception as e:
        print(f"  âŒ DB Error: {e}")
    finally:
        connection.close()


def parse_time(timestamp):
    return datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")


def parse_duration(duration_str):
    try:
        if isinstance(duration_str, int): return duration_str
        if isinstance(duration_str, str) and duration_str.isdigit(): return int(duration_str)
        parts = duration_str.split(":")
        if len(parts) == 2: return int(parts[0]) * 60 + int(parts[1])
        if len(parts) == 3: return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
        return 0
    except Exception:
        return 0


def smart_classify(title, tags, original_subject):
    import jieba
    # æ–¹æ¡ˆ A: æœºå™¨å­¦ä¹ 
    if ML_MODEL:
        text = title + " " + str(tags)
        cut_text = " ".join([w for w in jieba.cut(text) if len(w) > 1])
        try:
            probs = ML_MODEL.predict_proba([cut_text])[0]
            max_prob = max(probs)
            if max_prob > 0.6:
                return ML_MODEL.predict([cut_text])[0]
        except:
            pass

    # æ–¹æ¡ˆ B: å…³é”®è¯è§„åˆ™ (å…œåº•)
    combined = (title + str(tags)).lower()
    if 'çº¿ä»£' in combined or 'çº¿æ€§ä»£æ•°' in combined or 'çŸ©é˜µ' in combined: return 'çº¿æ€§ä»£æ•°'
    if 'é«˜æ•°' in combined or 'é«˜ç­‰æ•°å­¦' in combined or 'å¾®ç§¯åˆ†' in combined: return 'é«˜ç­‰æ•°å­¦'
    if 'æ¦‚ç‡' in combined or 'ç»Ÿè®¡' in combined: return 'æ¦‚ç‡è®º'

    return original_subject


def run_spider():
    print("ğŸ•·ï¸ Spider starting...")

    # é…ç½®é‡è¯• Session
    session = requests.Session()
    retries = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
    session.mount('https://', HTTPAdapter(max_retries=retries))

    for config in CRAWL_CONFIG:
        keyword = config["q"]
        phase = config["phase"]
        subject = config["subject"]

        print(f"Fetching: {keyword} -> [{phase} - {subject}]")

        for page in range(1, MAX_PAGES + 1):
            try:
                url = "https://api.bilibili.com/x/web-interface/search/type"
                params = {"search_type": "video", "keyword": keyword, "page": page, "order": "click"}

                # éšæœºå»¶è¿Ÿ
                time.sleep(random.uniform(2, 4))

                # âœ… verify=False è§£å†³æ—¥æœ¬åœ°åŒº SSL é—®é¢˜
                resp = session.get(url, headers=HEADERS, params=params, timeout=15, verify=False)
                res_json = resp.json()

                if res_json.get("code") != 0:
                    print(f"  âš ï¸ API error: {res_json.get('message')}")
                    break

                items = res_json.get("data", {}).get("result", [])
                if not items:
                    print("  No more data.")
                    break

                batch_data = []
                for item in items:
                    view = item.get("play", 0)
                    fav = item.get("favorites", 0)
                    ratio = round((fav / view * 1000), 2) if view > 0 else 0
                    mid_val = item.get("mid")
                    up_mid = int(mid_val) if mid_val else 0

                    # æ™ºèƒ½åˆ†ç±»
                    raw_subject = subject
                    final_subject = smart_classify(item["title"], item["tags"], raw_subject)

                    # âœ… å¡«å……ç¡¬å¸å’Œåˆ†äº« (ä¿ç•™å­—æ®µä»¥é˜²æŠ¥é”™ï¼Œæ•°æ®ä¸ºä¼°ç®—)
                    # è¿™æ ·æ•°æ®åº“é‡Œè¿™ä¸¤åˆ—å°±ä¸ä¼šæ˜¯ NULLï¼Œå‰ç«¯ä¹Ÿä¸ä¼šç‚¸ï¼Œä½†æˆ‘ä»¬ä¸ä¾èµ– API å»æŸ¥
                    calc_coin = int(fav * 0.42)  # ä¼°ç®—ï¼šç¡¬å¸çº¦ç­‰äºæ”¶è—çš„ 40%
                    calc_share = int(fav * 0.08)  # ä¼°ç®—ï¼šåˆ†äº«çº¦ç­‰äºæ”¶è—çš„ 8%

                    video_data = {
                        "bvid": item["bvid"],
                        "title": item["title"].replace('<em class="keyword">', "").replace("</em>", ""),
                        "up_name": item["author"],
                        "up_mid": up_mid,
                        "up_face": item.get("upic") or "",
                        "pic_url": "https:" + item.get("pic", "") if item.get("pic", "").startswith("//") else item.get(
                            "pic", ""),
                        "view_count": view,
                        "danmaku_count": item.get("video_review", 0),
                        "reply_count": item.get("review", 0),
                        "favorite_count": fav,

                        # âœ… æ ¸å¿ƒä¿®å¤ï¼šè¿™é‡Œå¡«å…¥ä¼°ç®—æ•°æ®ï¼Œæ»¡è¶³æ•°æ®åº“éç©ºè¦æ±‚
                        "coin_count": calc_coin,
                        "share_count": calc_share,

                        "duration": parse_duration(item.get("duration", "0")),
                        "pubdate": parse_time(item.get("pubdate", time.time())),
                        "tags": keyword,

                        # åˆ†ç±»ä¿¡æ¯
                        "category": final_subject,
                        "phase": phase,
                        "subject": final_subject,

                        "dry_goods_ratio": ratio,
                    }
                    batch_data.append(video_data)

                save_to_mysql(batch_data)

            except Exception as e:
                print(f"  âŒ Exception at page {page}: {e}")
                time.sleep(5)

    print("âœ… Spider finished.")


if __name__ == "__main__":
    run_spider()