import requests
import time
import random
import pymysql
from datetime import datetime

# ================== ğŸ”´ é…ç½®åŒºåŸŸ ==================

# 1. Bç«™ Cookie (è¯·ç¡®ä¿è¿™é‡Œæ˜¯ä½ æœ€æ–°çš„ Cookie)
COOKIE = """buvid3=5FE1AD61-24A7-EFF1-ADC1-B601351A64B045266infoc; b_nut=1762067345; _uuid=C10A65D4C-7109E-1018D-B39E-962E5A645310947037infoc; buvid4=5C8A9777-82F4-8E73-D1FB-4562D5C89E2E81922-025101318-YrurpcNiUaxvNzgYzwCyJQ%3D%3D; buvid_fp=71eb915647f3446ab6704685cc0aa13e; rpdid=|(umRY)|JmYl0J'u~Yk|Y~J~u; DedeUserID=288417099; DedeUserID__ckMd5=c6f4cb34e9cb5b5b; theme-tip-show=SHOWED; theme-avatar-tip-show=SHOWED; theme-switch-show=SHOWED; CURRENT_QUALITY=127; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NjM3Mzk2ODIsImlhdCI6MTc2MzQ4MDQyMiwicGx0IjotMX0.zFDPBxxam6zdMQe4ELzrhAYvDbhvqmX0HhiDC2ViZkU; bili_ticket_expires=1763739622; SESSDATA=702fe139%2C1779032482%2Cf346b%2Ab2CjAKa2WIsxDae9veT0e59O9aBJexgkGp675DXcFC5J_Ac7-xVqcL35OjaJBLncMSgGESVkxrQ19KM1FjaW1qbmR2SWJnMXdCcV83LUJQcno3a3FTdFBWMFQxdjdMRHdQbnlucTc1S1lQcWZZV2Y5aU1qcXRfOVBwSUFkbjZwbW5abDZHenlDMzlBIIEC; bili_jct=c6c3fe1e61333978db3a6d650d9f7adf; sid=6t5hrhil; theme_style=dark; bp_t_offset_288417099=1137639001650364416; b_lsid=10A1D93108_19AA5E29422; bmg_af_switch=1; bmg_src_def_domain=i0.hdslb.com; home_feed_column=4; browser_resolution=616-954; CURRENT_FNVAL=4048"""

# 2. æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': '123456',
    'db': 'bilibili_math_db',
    'charset': 'utf8mb4',
    'cursorclass': pymysql.cursors.DictCursor
}

# 3. âœ… å‡çº§åçš„æœç´¢æ˜ å°„ (æ–°å¢äº†ä¹ é¢˜ã€çœŸé¢˜ç­‰åˆ†ç±»)
SEARCH_MAP = {
    # --- æ•™æåŒæ­¥ç±» ---
    'å®‹æµ© é«˜ç­‰æ•°å­¦': 'é«˜æ•°(æ•™æç‰ˆ)',
    'å¼ å®‡ é«˜ç­‰æ•°å­¦': 'é«˜æ•°(å¼ºåŒ–)',
    'æ±¤å®¶å‡¤ é«˜æ•°': 'é«˜æ•°(åŸºç¡€)',

    # --- å­¦ç§‘ç»†åˆ†ç±» ---
    'ææ°¸ä¹ çº¿æ€§ä»£æ•°': 'çº¿æ€§ä»£æ•°',
    'å®‹æµ© æ¦‚ç‡è®º': 'æ¦‚ç‡ç»Ÿè®¡',
    '3Blue1Brown': 'æ•°å­¦ç§‘æ™®',  # è¿™ç§å±äºæ‰©å±•è§†é‡
    'æ­¦å¿ ç¥¥ é«˜æ•°': 'é«˜æ•°(å¼ºåŒ–)',     # è€ƒç ”ä¸‰å¤§å·¨å¤´ä¹‹ä¸€
    'ä½™ä¸™æ£® æ¦‚ç‡è®º': 'æ¦‚ç‡ç»Ÿè®¡',     # æ¦‚ç‡è®ºåå¸ˆ
    'å§œæ™“åƒ é«˜æ•°': 'é«˜æ•°(é€šä¿—)',     # é€‚åˆåŸºç¡€å·®çš„
    'å‘¨æ´‹é‘« é«˜æ•°': 'é«˜æ•°(æŠ€å·§)',     # æŠ€å·§æµ
    'æ¨è¶… é«˜æ•°': 'é«˜æ•°(åŸºç¡€)',       # åŸºç¡€æµ
    'è€ƒç ”æ•°å­¦ è¿™é‡Œçš„é»æ˜é™æ‚„æ‚„': 'ä¹ é¢˜è®²è§£', # çŸ¥åUPä¸»
    'ç‹è°± æ¦‚ç‡è®º': 'æ¦‚ç‡ç»Ÿè®¡',

    # --- è€ƒè¯•ä¸ä¹ é¢˜ç±» (è¿™æ˜¯å­¦ç”Ÿæœ€å…³å¿ƒçš„ï¼) ---
    'è€ƒç ”æ•°å­¦ çœŸé¢˜': 'çœŸé¢˜å®æˆ˜',
    'æ¥åŠ›é¢˜å…¸ 1800': 'åˆ·é¢˜ç‰¹è®­',
    'è€ƒç ”æ•°å­¦ å†²åˆº': 'è€ƒå‰å†²åˆº',
    'ä¸“å‡æœ¬ é«˜æ•°': 'ä¸“å‡æœ¬ä¸“åŒº'
}

MAX_PAGES = 5  # æ¼”ç¤ºç”¨ï¼ŒæŠ“å– 5 é¡µ

# ==================================================

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Referer': 'https://www.bilibili.com/',
    'Cookie': COOKIE
}


def save_to_mysql(data_list):
    if not data_list: return
    connection = pymysql.connect(**DB_CONFIG)
    try:
        with connection.cursor() as cursor:
            sql = """
            INSERT INTO videos (
                bvid, title, up_name, pic_url, view_count, danmaku_count, 
                reply_count, favorite_count, coin_count, share_count, 
                duration, pubdate, tags, category, dry_goods_ratio
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
                view_count = VALUES(view_count),
                favorite_count = VALUES(favorite_count),
                dry_goods_ratio = VALUES(dry_goods_ratio),
                category = VALUES(category); -- æ›´æ–°æ—¶ä¹Ÿæ›´æ–°åˆ†ç±»
            """
            values = []
            for item in data_list:
                values.append((
                    item['bvid'], item['title'], item['up_name'], item['pic_url'],
                    item['view_count'], item['danmaku_count'], item['reply_count'],
                    item['favorite_count'], item['coin_count'], item['share_count'],
                    item['duration'], item['pubdate'], item['tags'],
                    item['category'], item['dry_goods_ratio']
                ))
            cursor.executemany(sql, values)
            connection.commit()
            print(f"   âœ… å…¥åº“/æ›´æ–° {len(data_list)} æ¡ - åˆ†ç±»: {data_list[0]['category']}")
    except Exception as e:
        print(f"   âŒ æ•°æ®åº“é”™è¯¯: {e}")
    finally:
        connection.close()


def parse_time(timestamp):
    return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d %H:%M:%S')


def parse_duration(duration_str):
    try:
        if isinstance(duration_str, int):
            return duration_str
        if isinstance(duration_str, str) and duration_str.isdigit():
            return int(duration_str)

        parts = duration_str.split(':')
        if len(parts) == 2:
            return int(parts[0]) * 60 + int(parts[1])
        elif len(parts) == 3:
            return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
        return 0
    except:
        return 0


def run_spider():
    print("ğŸš€ çˆ¬è™«å¯åŠ¨...")
    for keyword, category in SEARCH_MAP.items():
        print(f"\nğŸ” æ­£åœ¨æŠ“å–: {keyword} -> [{category}]")
        for page in range(1, MAX_PAGES + 1):
            try:
                url = 'https://api.bilibili.com/x/web-interface/search/type'
                params = {'search_type': 'video', 'keyword': keyword, 'page': page, 'order': 'click'}
                resp = requests.get(url, headers=HEADERS, params=params, timeout=10)
                res_json = resp.json()

                if res_json['code'] != 0:
                    print(f"   âš ï¸ æ¥å£æŠ¥é”™: {res_json.get('message')}")
                    break

                items = res_json['data']['result']
                if not items: break

                batch_data = []
                for item in items:
                    view = item.get('play', 0)
                    fav = item.get('favorites', 0)
                    ratio = round((fav / view * 1000), 2) if view > 0 else 0

                    video_data = {
                        'bvid': item['bvid'],
                        'title': item['title'].replace('<em class="keyword">', '').replace('</em>', ''),
                        'up_name': item['author'],
                        'pic_url': item.get('pic', ''),
                        'view_count': view,
                        'danmaku_count': item.get('video_review', 0),
                        'reply_count': item.get('review', 0),
                        'favorite_count': fav,
                        'coin_count': 0, 'share_count': 0,
                        'duration': parse_duration(item.get('duration', '0')),
                        'pubdate': parse_time(item.get('pubdate', time.time())),
                        'tags': keyword,
                        'category': category,
                        'dry_goods_ratio': ratio
                    }
                    batch_data.append(video_data)
                save_to_mysql(batch_data)
                time.sleep(random.uniform(2, 3))
            except Exception as e:
                print(f"   âŒ å¼‚å¸¸: {e}")
    print("\nğŸ‰ å…¨éƒ¨å®Œæˆï¼")


if __name__ == '__main__':
    run_spider()